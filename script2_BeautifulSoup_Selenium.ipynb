{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importing necessary libraries\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from constants import LINKEDIN_USER_EMAIL, LINKEDIN_USER_PASS\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\AppData\\Local\\Temp\\ipykernel_27708\\2909792738.py:2: DeprecationWarning: executable_path has been deprecated, please pass in a Service object\n",
      "  driver = webdriver.Chrome(\"chromedriver.exe\")\n"
     ]
    }
   ],
   "source": [
    "# Creating an instance of Chrome WebDriver\n",
    "## WebDriver Version is 120\n",
    "driver = webdriver.Chrome(\"chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## A function to scroll down the web-page\n",
    "def scroll_down():\n",
    "    start = time.time()\n",
    "\n",
    "    # will be used in the while loop\n",
    "    initialScroll = 0\n",
    "    finalScroll = 1000\n",
    "\n",
    "    while True:\n",
    "        driver.execute_script(f\"window.scrollTo({initialScroll}, {finalScroll})\")\n",
    "        # this command scrolls the window starting from\n",
    "        # the pixel value stored in the initialScroll \n",
    "        # variable to the pixel value stored at the\n",
    "        # finalScroll variable\n",
    "        initialScroll = finalScroll\n",
    "        finalScroll += 1000\n",
    "        # we will stop the script for 3 seconds so that \n",
    "        # the data can load\n",
    "        time.sleep(3)\n",
    "        # You can change it as per your needs and internet speed\n",
    "        end = time.time()\n",
    "        # We will scroll for 20 seconds.\n",
    "        # You can change it as per your needs and internet speed\n",
    "        if round(end - start) > 20:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\requests\\__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.9) or chardet (5.1.0)/charset_normalizer (2.0.12) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "## Importing Necessary Libraries and creating an instance of Linkedin Class to get the links from a search query\n",
    "from linkedin_api import Linkedin\n",
    "from constants import LINKEDIN_USER_EMAIL, LINKEDIN_USER_PASS\n",
    "\n",
    "linkedin_api = Linkedin(LINKEDIN_USER_EMAIL, LINKEDIN_USER_PASS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Assuming first and last names of any uset is given\n",
    "First_name = 'YOUR_FIRST_NAME'\n",
    "Last_name = 'YOUR_LAST_NAME'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Creating the name (in lower case)\n",
    "name = First_name + ' ' + Last_name\n",
    "name = name.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Searching for the profile with the keyword as the name\n",
    "profiles = linkedin_api.search(params={'keywords': name}, limit=1)\n",
    "## Creating users' table\n",
    "df = pd.DataFrame(profiles)\n",
    "no_of_profiles = 10\n",
    "## Selecting top 10 links of linkedin profiles from the list\n",
    "links_of_profiles = list(df['navigationUrl'][:no_of_profiles])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles_data = []\n",
    "# Logging into LinkedIn\n",
    "driver.get(\"https://linkedin.com/uas/login\")\n",
    "time.sleep(5)\n",
    "\n",
    "## Entering the Email ID\n",
    "username = driver.find_element(By.ID, \"username\")\n",
    "username.send_keys(LINKEDIN_USER_EMAIL)\n",
    "## Entering the Password\n",
    "pword = driver.find_element(By.ID, \"password\")\n",
    "pword.send_keys(LINKEDIN_USER_PASS)\t # Enter Your Password\n",
    "## Submitting the login request\n",
    "driver.find_element(By.XPATH, \"//button[@type='submit']\").click()\n",
    "## Going through each profile\n",
    "for link in links_of_profiles :\n",
    "    data = {}\n",
    "    # Trying to Open Profile\n",
    "    try :\n",
    "        profile_url = link\n",
    "        driver.get(profile_url)\t # this will open the link\n",
    "        scroll_down()\t # this will scroll down the page\n",
    "\n",
    "        src = driver.page_source\n",
    "        # Now using beautiful soup\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        # Extracting the HTML of the complete introduction box\n",
    "        # that contains the name, company name, and the location\n",
    "        ## name\n",
    "        name = soup.find('h1', {'class' : 'text-heading-xlarge inline t-24 v-align-middle break-words'})\n",
    "        data['name'] = name.text if name is not None else np.nan \n",
    "        ## Headline\n",
    "        tag = soup.find('div', {'class': 'text-body-medium break-words'})\n",
    "        data['headline'] = tag.text.replace('\\n', '').strip() if tag is not None else np.nan\n",
    "        ## Place\n",
    "        loc = soup.find('span', {'class': 'text-body-small inline t-black--light break-words'})\n",
    "        data['location'] = loc.text.replace('\\n', '').strip() if loc is not None else np.nan\n",
    "        ## About\n",
    "        about = soup.find('div', {'class': 'display-flex ph5 pv3'})\n",
    "        data['about'] = about.span.text.replace('\\n', '').strip() if about is not None else np.nan\n",
    "    except Exception as e:\n",
    "        print(\"Profile not found...\")\n",
    "        data['name'] = np.nan\n",
    "        data['headline'] = np.nan\n",
    "        data['location'] = np.nan\n",
    "        data['about'] = np.nan\n",
    "\n",
    "    ## Trying to Scrape Experiences\n",
    "    try :\n",
    "        experience_url = profile_url + \"details/experience/\"\n",
    "        driver.get(experience_url)  # this will open the link\n",
    "        scroll_down()\t # this will scroll down the page\n",
    "        src = driver.page_source\n",
    "        # Now using beautiful soup\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        # Extracting the HTML of the complete experiece box\n",
    "        # that contains experiences\n",
    "        roles = soup.findAll('div', {'class': 'display-flex align-items-center mr1 t-bold'})\n",
    "        companies = soup.findAll('span', {'class': 't-14 t-normal'})\n",
    "        dates = soup.findAll('span', {'class': 'pvs-entity__caption-wrapper'})\n",
    "        ## Storing them in a list of dictionaries format\n",
    "        final_experiences = []\n",
    "        for i in range(0, len(roles)) :\n",
    "            experiences = {}\n",
    "            experiences['role'] = (roles[i].span.text if len(roles) > i else '')\n",
    "            experiences['company'] = (companies[i].span.text.replace('\\n', '').strip() if len(companies) > i else '')\n",
    "            experiences['date'] = (dates[i].text if len(dates) > i else '')\n",
    "            final_experiences.append(experiences)\n",
    "            \n",
    "        data['experiences'] = [final_experiences]\n",
    "    except Exception as e:\n",
    "        print(\"Experiences not found...\")\n",
    "        data['experiences'] = [np.nan]\n",
    "\n",
    "    ## Trying to Scrape Education\n",
    "    try :\n",
    "        education_url = profile_url + \"details/education/\"\n",
    "        driver.get(education_url)\t # this will open the link\n",
    "        scroll_down()\t # this will scroll down the page\n",
    "\n",
    "        src = driver.page_source\n",
    "        # Now using beautiful soup\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        # Extracting the HTML of the education box\n",
    "        schools = soup.findAll('div', {'class': 'display-flex align-items-center mr1 hoverable-link-text t-bold'})\n",
    "        degrees = soup.findAll('span', {'class': 't-14 t-normal'})\n",
    "        dates = soup.findAll('span', {'class': 'pvs-entity__caption-wrapper'})\n",
    "\n",
    "        final_education = []\n",
    "        for i in range(0, len(schools)) :\n",
    "            education = {}\n",
    "            education['role'] = (schools[i].span.text if len(schools) > i else '')\n",
    "            education['company'] = (degrees[i].span.text.replace('\\n', '').strip() if len(degrees) > i else '')\n",
    "            education['date'] = (dates[i].text if len(dates) > i else '')\n",
    "            final_education.append(education)\n",
    "            \n",
    "        data['experiences'] = [final_education]\n",
    "    except Exception as e:\n",
    "        print(\"Education not found...\")\n",
    "        data['education'] = [np.nan]\n",
    "\n",
    "    ## Trying to Scrape skills section\n",
    "    try :\n",
    "        skill_url = profile_url + 'details/skills/'\n",
    "\n",
    "        driver.get(skill_url)\t # this will open the link\n",
    "        scroll_down()\t # this will scroll down the page\n",
    "\n",
    "        src = driver.page_source\n",
    "        # Now using beautiful soup\n",
    "        soup = BeautifulSoup(src, 'lxml')\n",
    "        # Extracting the HTML of the skills box\n",
    "        skill_elements = soup.findAll('div', {'class': 'display-flex flex-row justify-space-between'})\n",
    "        skills = []\n",
    "        for i in range(0, len(skill_elements)) :\n",
    "            skills.append(skill_elements[i].span.text)\n",
    "\n",
    "        data['skills'] = [skills]\n",
    "    except Exception as e:\n",
    "        print(\"Skills not found...\")\n",
    "        data['skills'] = [np.nan]\n",
    "    ## Storing the data of a person in a list\n",
    "    profiles_data.append(data)\n",
    "    print(\"Finished Scraping A Profile...!!!\")\n",
    "    ## Done with a profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Storing the data in local\n",
    "linkedin_profiles_data = pd.DataFrame(profiles_data)\n",
    "filename = First_name + '_' + Last_name + '.csv'\n",
    "linkedin_profiles_data.to_csv(filename, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
